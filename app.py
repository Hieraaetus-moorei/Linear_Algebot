# import flask-related modules
from flask import Flask, request, abort, send_from_directory

# requests module for server communication
import requests

# import line bot sdk for python
from linebot.v3 import (
    WebhookHandler
)
from linebot.v3.exceptions import (
    InvalidSignatureError
)
from linebot.v3.messaging import (
    Configuration,
    ApiClient,
    MessagingApi,
    ReplyMessageRequest,
    TextMessage,
    ImageMessage,
    VideoMessage,
    AudioMessage
)
from linebot.v3.webhooks import (
    MessageEvent,
    TextMessageContent,
    ImageMessageContent,
    VideoMessageContent,
    AudioMessageContent
)



#======================== bot's brain ========================#

from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch
from sentence_transformers import SentenceTransformer, util
import faiss

# reduce the precision for memory and calculation efficiency
if torch.cuda.is_available():
    bnb_config = BitsAndBytesConfig(
        load_in_8bit = True,
        llm_int8_enable_fp32_cpu_offload = True  # enable CPU offload
    )

# load language model and tokeniser
model_name = 'microsoft/Phi-3.5-mini-instruct' # You can replace it with any other model you like

# set a pretrainmodel as a tokeniser; use_fast = True: Rust implementation for efficiency
tokeniser = AutoTokenizer.from_pretrained(model_name, use_fast = True) # set it to False will lead to python implementation
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config = bnb_config if torch.cuda.is_available() else None,
    # load_in_8bit = True,  # use 8-bit quantisation for memory efficiency
    device_map = 'auto' if torch.cuda.is_available() else 'cpu',  # force to use cpu if there isn't CUDA
)

# placeholder for the knowledge base (to be replaced with your actual data)
knowledge_base = [
    {'text': 'The capital of France is Paris.'},
    {'text': 'The Eiffel Tower is located in Paris.'},
    {'text': 'Python is a popular programming language.'},
    {'text': 'Fortran is regaining its popularity again'},
    {'text': 'Open source is the best idea in software development.'},
    {'text': 'Scihub is a great tool for academic research.'}
]

# create embeddings for the knowledge base
embedder = SentenceTransformer('all-mpnet-base-v2')
knowledge_embeddings = embedder.encode([item['text'] for item in knowledge_base])

# build the FAISS index
index = faiss.IndexFlatL2(knowledge_embeddings.shape[1])
index.add(knowledge_embeddings)

# define a function to retrieve the most similar data from knowledge base
def get_relevant_documents(query, top_k = 1): # top_k is the number of relevant documents to retrieve
    # encode the query into embedding
    query_embedding = embedder.encode(query)
    # Distance and Indices from FAISS
    D, I = index.search(query_embedding.reshape(1, -1), k = top_k)
    # retrieve the relevant documents
    relevant_docs = [knowledge_base[i] for i in I[0]]
    return relevant_docs

# define the chatbot function
def chat_with_bot(user_input):
    # using RAG to map the question with answer in knowledge base
    relevant_docs = get_relevant_documents(user_input)
    context = '\n'.join([doc['text'] for doc in relevant_docs])
    
    # customising your own prompt here
    prompt = f'''You are a helpful assistant, and you must answer the question 'BRIEFLY' in the same language with question.
    
    info: {context}

    Question: {user_input}
    '''
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    inputs = tokeniser(prompt, return_tensors='pt').to(device) # force to use cpu if cuda is unavailable
    # using pytorch for inference without remembering the gradient calculation for memory efficiency
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens = 50) # set the max length of the output
    response = tokeniser.decode(outputs[0], skip_special_tokens = True)

    # Ensure prompt has a newline character
    # Remove the prompt from the response
    response = response[len(prompt):].replace('Answer: ', '')

    # Split the response by newline characters
    parts = response.split('\n', 2)

    # If there are at least two newline characters, remove the content after the second one
    if len(parts) > 2:
        response = parts[0] + '\n' + parts[1]

    print(response)
    return response.strip()

#======================== cerebrum ========================#



# instantiate the flask app
app = Flask(__name__)

# setting up the LINE Bot access token and secret in config.py
from config import Config
config = Config()
configuration = Configuration(access_token = config['YOUR_CHANNEL_ACCESS_TOKEN'])
handler = WebhookHandler(config['YOUR_CHANNEL_SECRET'])



# Paste the url generated by tunnelmole (or localtunnel) here
prefix_url = ''



'''
route for the webhook
'''
# configuring the route for the webhook on Line developer console
@app.route('/callback', methods = ['POST'])
def callback():
    # get X-Line-Signature header value
    signature = request.headers['X-Line-Signature']

    # get request body as text
    body = request.get_data(as_text = True)
    app.logger.info('Request body: ' + body)

    # handle webhook body
    try:
        handler.handle(body, signature)
    except InvalidSignatureError:
        app.logger.info('Invalid signature. Please check your channel access token/channel secret.')
        abort(400)

    return 'OK'

# set a root route just for test
@app.route('/', methods = ['POST', 'GET'])
def index_page():
    return 'Hello, this is your LINE bot\'s homepage!'

# set a route for file upload
@app.route('/files/<path>')
def get_tmp_path(path):
    return send_from_directory('files', path)

# handling the text message
@handler.add(MessageEvent, message = TextMessageContent)
def handle_text_message(event):
    print('successfully get into text message handler')
    with ApiClient(configuration) as api_client:
        print('successfully get into api client')
        # instantiate message api object
        api_instance  = MessagingApi(api_client)

        # get reply token
        replyToken = event.reply_token

        # get message context
        replyText = event.message.text

        # prepare the messages for reply (max: 5)
        list_reply = [
            # TextMessage(text = 'just a test'), # text message for test
            # TextMessage(text = replyText), # echo the user's message
            TextMessage(text = chat_with_bot(replyText)), # chatbot answer
        ]

        # reply to the user via Line Webhook
        api_instance.reply_message_with_http_info(
            ReplyMessageRequest(
                reply_token = replyToken,
                messages = list_reply,
            )
        )

# image message handler
@handler.add(MessageEvent, message = ImageMessageContent)
def handle_image_message(event):
    with ApiClient(configuration) as api_client:
        # instantiate message api object
        api_instance  = MessagingApi(api_client)

        # get replyToken
        replyToken = event.reply_token
        
        # get message ID
        messageId = event.message.id

        # set request header
        my_headers = {
            'Authorization': f'Bearer {config["YOUR_CHANNEL_ACCESS_TOKEN"]}',
        }

        url = f'https://api-data.line.me/v2/bot/message/{messageId}/content'

        # request sending
        response = requests.get(url = url, headers = my_headers)

        # save the image (response.content is binary data)
        with open(f'files/{messageId}.jpg', 'wb') as file:
            file.write(response.content)

        replyText = f'image received, {messageId}.jpg'

        # prepare the messages for reply (max: 5)
        list_reply = [
            TextMessage(text = replyText),
            ImageMessage(original_content_url = f'{prefix_url}/files/{messageId}.jpg', preview_image_url = f'{prefix_url}/files/{messageId}.jpg')
        ]

        # reply to the user via Line Webhook
        api_instance.reply_message_with_http_info(
            ReplyMessageRequest(
                reply_token = replyToken,
                messages = list_reply
            )
        )

# video message handler
@handler.add(MessageEvent, message = VideoMessageContent)
def handle_video_message(event):
    with ApiClient(configuration) as api_client:
        # instantiate the API class
        api_instance = MessagingApi(api_client)

        # get the reply token
        replyToken = event.reply_token
        
        # get message id
        messageId = event.message.id

        # set the request header
        my_headers = {
            'Authorization': f'Bearer {config["YOUR_CHANNEL_ACCESS_TOKEN"]}',
        }

        url = f'https://api-data.line.me/v2/bot/message/{messageId}/content'

        # execute the request
        response = requests.get(url = url, headers = my_headers)

        # save the video (response.content is binary data)
        with open(f'files/{messageId}.mp4', 'wb') as file:
            file.write(response.content)

        replyText = f'video received, {messageId}.mp4'

        # prepare the messages for reply (max: 5)
        list_reply = [
            TextMessage(text = replyText),
            VideoMessage(original_content_url = f'{prefix_url}/files/{messageId}.mp4', preview_image_url = f'{prefix_url}/files/{messageId}.mp4')
        ]

        # send the reply messages
        api_instance.reply_message_with_http_info(
            ReplyMessageRequest(
                reply_token = replyToken,
                messages = list_reply
            )
        )

# audio message handler
@handler.add(MessageEvent, message = AudioMessageContent)
def handle_audio_message(event):
    with ApiClient(configuration) as api_client:
        # instantiate the API class
        api_instance  = MessagingApi(api_client)

        # get the reply token
        replyToken = event.reply_token
        
        # get message id
        messageId = event.message.id

        # set request header
        my_headers = {
            'Authorization': f'Bearer {config["YOUR_CHANNEL_ACCESS_TOKEN"]}',
        }

        url = f'https://api-data.line.me/v2/bot/message/{messageId}/content'

        # request execution
        response = requests.get(url = url, headers = my_headers)

        # save the audio file (response.content is binary data)
        with open(f'files/{messageId}.m4a', 'wb') as file:
            file.write(response.content)

        replyText = f'audio received, {messageId}.m4a'

        # replying messages preparation (max: 5)
        list_reply = [
            TextMessage(text = replyText),
            AudioMessage(original_content_url = f'{prefix_url}/files/{messageId}.m4a', duration = 1000)
        ]

        # send the reply messages
        api_instance.reply_message_with_http_info(
            ReplyMessageRequest(
                reply_token = replyToken,
                messages = list_reply
            )
        )



#====================================== execution ======================================#



app.run(
    debug = True,     # open debug mode
    host = '0.0.0.0', # serve at 0.0.0.0
    port = 1989       # use port 1989 (or any port you prefer)
)
    
    # if you have your own SSL certs, uncomment the following setting and modify the ssl_context value
    # app.run(
    #     debug = True,     # open debug mode
    #     host = '0.0.0.0', # serve at 0.0.0.0
    #     port = 1989,      # use port 1989 (or any port you prefer)
    #     # ssl_context = ('/certs/fullchain4.pem', '/certs/privkey4.pem') # set SSL certification for LINE Bot Webhook
    # )
